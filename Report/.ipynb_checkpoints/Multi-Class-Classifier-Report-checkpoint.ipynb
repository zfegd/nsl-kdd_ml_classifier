{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report on Multi-Class Classifiers Using NSL-KDD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Comparing Algorithms and Evaluation Metrics\n",
    "\n",
    "I will first explore the performance of 3 different algorithms (Random Forest, Linear SVC, and logistic regression) when using all the attributes as features, with regards to different evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set = pd.read_csv('../../KDDTrain+.csv', header=None)\n",
    "\n",
    "field_names = pd.read_csv('../../Field Names.csv', header=None)\n",
    "\n",
    "field_names_extension = pd.DataFrame([['attack_type', 'symbolic'],['??','continuous']])\n",
    "field_names = field_names.append(field_names_extension, ignore_index=True)\n",
    "\n",
    "cont_feats = field_names.loc[field_names[1] == 'continuous']\n",
    "cont_feats = cont_feats.drop(columns=[1])\n",
    "\n",
    "field_names = field_names.drop(columns=[1])\n",
    "\n",
    "dataframe_headers = field_names[0].tolist()\n",
    "train_set.columns = dataframe_headers\n",
    "\n",
    "cols = train_set.columns.tolist()\n",
    "cols = cols[-2:-1] + cols[:-2] + cols[-1:]\n",
    "train_set = train_set[cols]\n",
    "\n",
    "att_types = train_set['attack_type']\n",
    "\n",
    "# train_set.sort_values('attack_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to achieve such exploration, I would have to enumerate the categoricals (service, protocol type, and flag) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENUMERATING SERVICE TYPE\n",
    "import copy\n",
    "\n",
    "service_list = train_set['service'].unique()\n",
    "\n",
    "try:\n",
    "    service_list2\n",
    "except NameError:\n",
    "    service_list2 = copy.deepcopy(service_list)    \n",
    "    svc_enum_data = {'service': service_list}\n",
    "    service_enumerations = pd.DataFrame(data=svc_enum_data)\n",
    "    \n",
    "enumerated_service_list = [x for x in range(len(service_list))]\n",
    "\n",
    "\n",
    "train_set = train_set.replace(service_list,enumerated_service_list)\n",
    "\n",
    "# ENUMERATING PROTOCOL TYPE\n",
    "\n",
    "prot_list = train_set['protocol_type'].unique()\n",
    "\n",
    "try:\n",
    "    prot_list2\n",
    "except NameError:\n",
    "    prot_list2 = copy.deepcopy(prot_list)    \n",
    "    prot_enum_data = {'protocol_type': prot_list}\n",
    "    prot_enumerations = pd.DataFrame(data=prot_enum_data)\n",
    "    \n",
    "enumerated_prot_list = [x for x in range(len(prot_list))]\n",
    "\n",
    "\n",
    "train_set = train_set.replace(prot_list,enumerated_prot_list)\n",
    "\n",
    "# ENUMERATING FLAG TYPE\n",
    "\n",
    "flag_list = train_set['flag'].unique()\n",
    "\n",
    "try:\n",
    "    flag_list2\n",
    "except NameError:\n",
    "    flag_list2 = copy.deepcopy(flag_list)    \n",
    "    flag_enum_data = {'flag': flag_list}\n",
    "    flag_enumerations = pd.DataFrame(data=flag_enum_data)\n",
    "    \n",
    "enumerated_flag_list = [x for x in range(len(flag_list))]\n",
    "\n",
    "\n",
    "train_set = train_set.replace(flag_list,enumerated_flag_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('../../KDDTest+.csv', header=None)\n",
    "test_set.columns = dataframe_headers\n",
    "\n",
    "test_set = test_set[cols]\n",
    "\n",
    "test_set = test_set.replace(service_list2,enumerated_service_list)\n",
    "test_set['service'] = test_set['service'].apply(lambda x: '-1' if type(x) is str else x)\n",
    "\n",
    "test_set = test_set.replace(prot_list2, enumerated_prot_list)\n",
    "test_set['protocol_type'] = test_set['protocol_type'].apply(lambda x: '-1' if type(x) is str else x)\n",
    "\n",
    "test_set = test_set.replace(flag_list2, enumerated_flag_list)\n",
    "test_set['flag'] = test_set['flag'].apply(lambda x: '-1' if type(x) is str else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Test and Train to perform Stratified Sampling\n",
    "Stratified sampling is used so as to ensure the fairness in the distribution. This is to ensure that both the training and test set is representative of the population, and reduce biases involved that may skew our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_table = train_set.append(test_set, ignore_index=True)\n",
    "combined_table_features = combined_table.drop(columns=['attack_type'])\n",
    "combined_table_target = combined_table['attack_type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_target, test_target = train_test_split(combined_table_features, combined_table_target, test_size=0.1, shuffle=True, stratify=combined_table_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_clf_lin = LinearSVC()\n",
    "svc_clf_lin.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing The Classifiers \n",
    "Micro is used due to class imbalance in multi class classification, and can be compared with the Weighted results for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "result_df_columns = ['Classifier', 'Accuracy', 'Precision (Micro)', 'Precision (Weighted)', 'Recall (Micro)', 'Recall (Weighted)', 'F-1 (Micro)', 'F1 (Weighted)']\n",
    "clf_cmp_dataframe = pd.DataFrame(columns=result_df_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " import warnings\n",
    " warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "# SVC linear\n",
    "svc_lin_pred_list = svc_clf_lin.predict(test_features)\n",
    "svc_lin_acc = accuracy_score(test_target, svc_lin_pred_list)\n",
    "svc_lin_prec_mic = precision_score(test_target, svc_lin_pred_list, average='micro')\n",
    "svc_lin_prec_mac = precision_score(test_target, svc_lin_pred_list, average='weighted')\n",
    "svc_lin_rec_mic = recall_score(test_target, svc_lin_pred_list, average='micro')\n",
    "svc_lin_rec_mac = recall_score(test_target, svc_lin_pred_list, average='weighted')\n",
    "svc_lin_f1_mic = f1_score(test_target, svc_lin_pred_list, average='micro')\n",
    "svc_lin_f1_mac = f1_score(test_target, svc_lin_pred_list, average='weighted')\n",
    "\n",
    "svc_lin_res = [\"SVC Linear\", svc_lin_acc, svc_lin_prec_mic, svc_lin_prec_mac, svc_lin_rec_mic, svc_lin_rec_mac, svc_lin_f1_mic, svc_lin_f1_mac]\n",
    "svc_lin_df = pd.DataFrame([svc_lin_res], columns=result_df_columns)\n",
    "\n",
    "clf_cmp_dataframe = clf_cmp_dataframe.append(svc_lin_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF\n",
    "\n",
    "rf_pred_list = rf_clf.predict(test_features)\n",
    "rf_acc = accuracy_score(test_target, rf_pred_list)\n",
    "rf_prec_mic = precision_score(test_target, rf_pred_list, average='micro')\n",
    "rf_prec_mac = precision_score(test_target, rf_pred_list, average='weighted')\n",
    "rf_rec_mic = recall_score(test_target, rf_pred_list, average='micro')\n",
    "rf_rec_mac = recall_score(test_target, rf_pred_list, average='weighted')\n",
    "rf_f1_mic = f1_score(test_target, rf_pred_list, average='micro')\n",
    "rf_f1_mac = f1_score(test_target, rf_pred_list, average='weighted')\n",
    "\n",
    "rf_res = [\"RF\", rf_acc, rf_prec_mic, rf_prec_mac, rf_rec_mic, rf_rec_mac, rf_f1_mic, rf_f1_mac]\n",
    "rf_df = pd.DataFrame([rf_res], columns=result_df_columns)\n",
    "\n",
    "clf_cmp_dataframe = clf_cmp_dataframe.append(rf_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR\n",
    "\n",
    "lr_pred_list = lr_clf.predict(test_features)\n",
    "lr_acc = accuracy_score(test_target, lr_pred_list)\n",
    "lr_prec_mic = precision_score(test_target, lr_pred_list, average='micro')\n",
    "lr_prec_mac = precision_score(test_target, lr_pred_list, average='weighted')\n",
    "lr_rec_mic = recall_score(test_target, lr_pred_list, average='micro')\n",
    "lr_rec_mac = recall_score(test_target, lr_pred_list, average='weighted')\n",
    "lr_f1_mic = f1_score(test_target, lr_pred_list, average='micro')\n",
    "lr_f1_mac = f1_score(test_target, lr_pred_list, average='weighted')\n",
    "\n",
    "lr_res = [\"LR\", lr_acc, lr_prec_mic, lr_prec_mac, lr_rec_mic, lr_rec_mac, lr_f1_mic, lr_f1_mac]\n",
    "lr_df = pd.DataFrame([lr_res], columns=result_df_columns)\n",
    "\n",
    "clf_cmp_dataframe = clf_cmp_dataframe.append(lr_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (Micro)</th>\n",
       "      <th>Precision (Weighted)</th>\n",
       "      <th>Recall (Micro)</th>\n",
       "      <th>Recall (Weighted)</th>\n",
       "      <th>F-1 (Micro)</th>\n",
       "      <th>F1 (Weighted)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC Linear</td>\n",
       "      <td>0.907016</td>\n",
       "      <td>0.907016</td>\n",
       "      <td>0.935756</td>\n",
       "      <td>0.907016</td>\n",
       "      <td>0.907016</td>\n",
       "      <td>0.907016</td>\n",
       "      <td>0.914159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.997307</td>\n",
       "      <td>0.997307</td>\n",
       "      <td>0.996695</td>\n",
       "      <td>0.997307</td>\n",
       "      <td>0.997307</td>\n",
       "      <td>0.997307</td>\n",
       "      <td>0.996968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.850458</td>\n",
       "      <td>0.850458</td>\n",
       "      <td>0.802226</td>\n",
       "      <td>0.850458</td>\n",
       "      <td>0.850458</td>\n",
       "      <td>0.850458</td>\n",
       "      <td>0.807103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Classifier  Accuracy  Precision (Micro)  Precision (Weighted)  \\\n",
       "0  SVC Linear  0.907016           0.907016              0.935756   \n",
       "1          RF  0.997307           0.997307              0.996695   \n",
       "2          LR  0.850458           0.850458              0.802226   \n",
       "\n",
       "   Recall (Micro)  Recall (Weighted)  F-1 (Micro)  F1 (Weighted)  \n",
       "0        0.907016           0.907016     0.907016       0.914159  \n",
       "1        0.997307           0.997307     0.997307       0.996968  \n",
       "2        0.850458           0.850458     0.850458       0.807103  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_cmp_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Evaluating Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to Choose\n",
    "\n",
    "As seen from the data above, the Random Forest model performs better than the other models in every category. This could be possible due to the fact that this dataset is a multi-class problem, which is not equally distributed amongst every class. Having an ensemble of decision trees to form a Random Forest allows for different cases to be explored, and build a better model to predict by. \n",
    "\n",
    "### Metrics to Choose\n",
    "\n",
    "As the data involved pertains to potential attacks on a network system, it is important to take necessary precautions to mitigate any potential damage. In such a situation, it may be beneficial to allow for more False Positives as compared to False Negatives. This is to ensure that any potential risks can be explored, where it is better to realise that there is nothing wrong as compared to not detecting an attack, and realising when it is too late. Hence, we want to have a high recall, where the proportions of False Negatives are as low as possible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Using K-Fold CV\n",
    "\n",
    "In order to ensure that the results obtained were representative, I have used K-Folds cross-validation to check if the results obtained are similar using different folds of the data instead.\n",
    "Cross-validation is necessary to validate the model on other sets of data than the training set to ensure as little bias, and K-folds cross-validation is one of the more popular ways to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9970306199676641"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k-fold cv\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "rf2_clf = RandomForestClassifier()\n",
    "\n",
    "np.mean(cross_val_score(rf2_clf, combined_table_features, combined_table_target, cv=kf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value obtained is very similar to the one above, which validates the appropriateness and accuracy of the RF classifier on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Iterations to Improve Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Data/ Pre-Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling\n",
    "I have chosen a MinMaxScaler to use, instead of the similar MaxAbsScaler. MinMaxScaler scales all features in the range [0,1], which helps the performance of certain models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9930649070832211"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "train_feats_scaled = scaler.fit_transform(train_features.values)\n",
    "train_features.iloc[:,:] = train_feats_scaled\n",
    "\n",
    "test_feats_scaled = scaler.fit_transform(test_features.values)\n",
    "test_features.iloc[:,:] = test_feats_scaled\n",
    "\n",
    "rf_scaler_clf = RandomForestClassifier()\n",
    "\n",
    "rf_scaler_clf.fit(train_features, train_target)\n",
    "rf_pred_list_scaler = rf_scaler_clf.predict(test_features)\n",
    "\n",
    "recall_score(test_target, rf_pred_list_scaler, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite scaling being supposed to help us pefrom better, this random classifier has ended up doing worse than the original in Section 1. Hence, this model can be discarded as it isn't as useful as the other. This is due to the fact that such scaling does not help decision-based tree models, such as the way Random Forest is implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Binary Classification on a Multi-Class Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the classifier's aim is to detect attacks from normal network accesses, we can modify the evaluation metrics to treat it as a binary classifier. This looser definition would allow for us to focus on attacks being identified, rather than the type of attack. This still serves our main purpose, where the broad goal is to distinguish attacks from normal accesses, to minimise the risks of attacks going undetected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.997621046739\n"
     ]
    }
   ],
   "source": [
    "# Calculating Recall of non-normal\n",
    "\n",
    "size_test_list = len(rf_pred_list)\n",
    "n_normal = 0\n",
    "retrieved_and_relevant = 0\n",
    "\n",
    "for x in range(size_test_list):\n",
    "    if test_target.values[x] == 'normal':\n",
    "        n_normal = n_normal + 1\n",
    "    elif rf_pred_list[x] != 'normal':\n",
    "        retrieved_and_relevant = retrieved_and_relevant + 1\n",
    "\n",
    "relevant = size_test_list - n_normal\n",
    "binary_recall = retrieved_and_relevant / float(relevant)\n",
    "print(binary_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value above shows that the multi-class model of the RF can be adopted to be treated as a binary classifier, as there is a high recall which minimises the number of attacks that go undetected. Albeit some of the attacks may be wrongly identified as another form of attack, the classifier would still flag them for investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Grid Search CV to Find Best RF Hyper-parameters\n",
    "While the default RF classifier performed extremely well already, I will use Grid Search CV to see if it can improve on the default model above. The default uses n_estimators at 10, and max_depth at None. These are the 2 hyper-parameters which I will be varying in order to observe the difference it has on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'n_estimators':[10,50,100,200], 'max_depth':[None,3,5,10,15,20,25,30,35,40]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 200, 'max_depth': 20}\n",
      "0.9973291237730428\n"
     ]
    }
   ],
   "source": [
    "rf_clf3 = RandomForestClassifier()\n",
    "cv_clf_2 = GridSearchCV(rf_clf3, parameters)\n",
    "cv_clf_2.fit(train_features, train_target)\n",
    "\n",
    "print(cv_clf_2.best_params_)\n",
    "print(cv_clf_2.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above shows a marginal gain in our model when we tweak the hyper-parameters passed into the Random Forest classifier. Having said that, a marginal gain is still an improvement that we can adopt to better our choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 - Limitations and Other Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Imbalanced Dataset : \n",
    "    The data set does not reflect the true nature of how network attacks take place, as the data represents close to a 50/50 split between \"normal\" and attacks. This would signify more of a binary or multi-class classifier, rather than an anomaly detector. In reality, the majority of network traffic should be \"normal\", which makes it harder for a true classifier to be able to be trained to do such detections of attack.\n",
    "    \n",
    "2) Clean Dataset :\n",
    "    The data set also contains clean data, where every field is filled-in. This does not accurately reflect real-life scenarios, where the data may have to undergo major pre-processing before sense can be made of it. The data also came in with mostly all the features labelled, which aids manual feature selection/creation.\n",
    "    \n",
    "3) Academic Dataset :\n",
    "    In addition to the points above about the dataset, it must also be noted that it is an academic dataset, that has been well-researched and explored previously. Even without any domain knowledge, the abundance of academic reports available online aided my ability to make better and well-informed decisions in terms of models to run, and what to expect.\n",
    "\n",
    "4) Choice of models :\n",
    "    In Section 1, I only considered Linear SVC, Random Forest, and Logistic Regressions, and this could have limited my results by not choosing other models to run as well. However, the costs of running more expensive models would not be worth the marginal gains over the results the Random Forest already obtained. The Linear SVC took 13 mins to fit and train the model, and this is already a lot quicker than if I did a SVC with a linear kernel instead. Hence, possibly better models such as SVC with RBF kernel and Deep Neural Networks were not considered in my exploration.\n",
    "\n",
    "5) Lack of Further Iterations :\n",
    "    Having found in Section 1 that Random Forest performs a fair bit better than Linear SVC and Logistic Regression, I chose to focus only on Random Forest in further iterations. Perhaps with further tweaking of the other models, improvements could have been found in the other models to become comparable with the Random Forest ones. Due to a lack of time and the costs of running GridSearchCV on models such as Linear SVC, I assumed that Random Forest would be the best model still, as it was already close to 100%.\n",
    "\n",
    "6) Ordinality Introduced in Enumeration :\n",
    "    When enumerating the categoricals in order to give them numerical values for the classifiers, a false ordinality was introduced in those features which may skew the result. The other option would be to use scikit-learn's one hot encoding, or numpy.eye, which would introduce the curse of dimensionality instead. \n",
    "    \n",
    "7) Feature Engineering :\n",
    "    As the results obtained via the Random Forest is already very good, there is little room for feature engineering, be it selection or creation, to improve the model. This might not be the case in other datasets, where methods such as recursive feature selection can be used to rank features. Other aspects of feature selection, such as correlation feature selection, or using the filter or wrapper method, can also be considered in other cases. Manual analysing and/or domain knowledge in other cases may also aid feature creation based on the data given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 - Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, I've found that Random Forest outperforms both the Linear SVC and Logistic Regression in creating a Multi-Class classifier to distinguish attacks in the NSL-KDD dataset. Through further iterations, there seemed to be only marginal gains as compared to the default model, as the default model had already performed really well (99.5%). It must also be noted that due to time and costs, other potentially better models such as the SVC with a RBF kernel were not tested. Given how well the default Random Forest has done though, it would suffice to just adopt it as the go-to model for this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
