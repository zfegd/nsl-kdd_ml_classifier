{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report on Multi-Class Classifiers Using NSL-KDD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Comparing Algorithms and Evaluation Metrics\n",
    "\n",
    "I will first explore the performance of 3 different algorithms (Random Forest, linear SVC, and logistic regression) when using all the attributes as features, with regards to different evaluation metrics. I have chosen these 3 algorithms as linear SVC and logistic regressions are both linear models that aare cost-effective and offer high intrepretability, while random forests relies on bootstrap aggregating to create an ensemble of decision trees that offer improved performance over a singular decision tree. If these models do not offer viable performance, I would consider other costlier but better models (such as SVC with RBF kernel) to implement instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "train_set = pd.read_csv('../../KDDTrain+.csv', header=None)\n",
    "field_names = pd.read_csv('../../Field Names.csv', header=None)\n",
    "\n",
    "# Adding headers for last 2 columns as they arrived unlabeled in the Field Names list\n",
    "# Unclear what the last column of data represents\n",
    "field_names_extension = pd.DataFrame([['attack_type', 'symbolic'],['??','continuous']])\n",
    "field_names = field_names.append(field_names_extension, ignore_index=True)\n",
    "\n",
    "# Dropping column which signifies if Continuous or Symbolic Attribute\n",
    "field_names = field_names.drop(columns=[1])\n",
    "\n",
    "# Adding headers to training set\n",
    "dataframe_headers = field_names[0].tolist()\n",
    "train_set.columns = dataframe_headers\n",
    "\n",
    "cols = train_set.columns.tolist()\n",
    "cols = cols[-2:-1] + cols[:-2] + cols[-1:]  #Re-ordering columns so that 'attack_type' is left-most column\n",
    "train_set = train_set[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to achieve such exploration, I would have to enumerate the categoricals (service, protocol type, and flag) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENUMERATING SERVICE TYPE\n",
    "\n",
    "service_list = train_set['service'].unique()\n",
    "\n",
    "# Try except to prevent overwrites in multiple iterations which would affect the results\n",
    "try:\n",
    "    service_list2\n",
    "except NameError:\n",
    "    service_list2 = copy.deepcopy(service_list)    \n",
    "    svc_enum_data = {'service': service_list}\n",
    "    service_enumerations = pd.DataFrame(data = svc_enum_data)\n",
    "    \n",
    "enumerated_service_list = [x for x in range(len(service_list))]\n",
    "\n",
    "train_set = train_set.replace(service_list, enumerated_service_list)\n",
    "\n",
    "# ENUMERATING PROTOCOL TYPE\n",
    "\n",
    "prot_list = train_set['protocol_type'].unique()\n",
    "\n",
    "try:\n",
    "    prot_list2\n",
    "except NameError:\n",
    "    prot_list2 = copy.deepcopy(prot_list)    \n",
    "    prot_enum_data = {'protocol_type': prot_list}\n",
    "    prot_enumerations = pd.DataFrame(data=prot_enum_data)\n",
    "    \n",
    "enumerated_prot_list = [x for x in range(len(prot_list))]\n",
    "\n",
    "train_set = train_set.replace(prot_list,enumerated_prot_list)\n",
    "\n",
    "# ENUMERATING FLAG TYPE\n",
    "\n",
    "flag_list = train_set['flag'].unique()\n",
    "\n",
    "try:\n",
    "    flag_list2\n",
    "except NameError:\n",
    "    flag_list2 = copy.deepcopy(flag_list)    \n",
    "    flag_enum_data = {'flag': flag_list}\n",
    "    flag_enumerations = pd.DataFrame(data=flag_enum_data)\n",
    "    \n",
    "enumerated_flag_list = [x for x in range(len(flag_list))]\n",
    "\n",
    "train_set = train_set.replace(flag_list,enumerated_flag_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('../../KDDTest+.csv', header=None)\n",
    "test_set.columns = dataframe_headers\n",
    "\n",
    "test_set = test_set[cols]\n",
    "\n",
    "test_set = test_set.replace(service_list2,enumerated_service_list)\n",
    "test_set['service'] = test_set['service'].apply(lambda x: '-1' if type(x) is str else x)\n",
    "\n",
    "test_set = test_set.replace(prot_list2, enumerated_prot_list)\n",
    "test_set['protocol_type'] = test_set['protocol_type'].apply(lambda x: '-1' if type(x) is str else x)\n",
    "\n",
    "test_set = test_set.replace(flag_list2, enumerated_flag_list)\n",
    "test_set['flag'] = test_set['flag'].apply(lambda x: '-1' if type(x) is str else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Test and Train to perform Stratified Sampling\n",
    "Stratified sampling is used so as to ensure the fairness in the distribution. This is to ensure that both the training and test set is representative of the population, and reduce biases involved that may skew our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_table = train_set.append(test_set, ignore_index=True)\n",
    "combined_table_features = combined_table.drop(columns=['attack_type'])\n",
    "combined_table_target = combined_table['attack_type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_target, test_target = train_test_split(combined_table_features, combined_table_target, test_size=0.1, shuffle=True, stratify=combined_table_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_clf_lin = LinearSVC()\n",
    "svc_clf_lin.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing The Classifiers \n",
    "Micro is used due to class imbalance in multi class classification, and can be compared with the Weighted results for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_columns = ['Classifier', 'Accuracy', 'Precision (Micro)', 'Precision (Weighted)', 'Recall (Micro)', 'Recall (Weighted)', 'F-1 (Micro)', 'F1 (Weighted)']\n",
    "clf_cmp_dataframe = pd.DataFrame(columns=result_df_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "# SVC linear\n",
    "svc_lin_pred_list = svc_clf_lin.predict(test_features)\n",
    "svc_lin_acc = accuracy_score(test_target, svc_lin_pred_list)\n",
    "svc_lin_prec_mic = precision_score(test_target, svc_lin_pred_list, average='micro')\n",
    "svc_lin_prec_mac = precision_score(test_target, svc_lin_pred_list, average='weighted')\n",
    "svc_lin_rec_mic = recall_score(test_target, svc_lin_pred_list, average='micro')\n",
    "svc_lin_rec_mac = recall_score(test_target, svc_lin_pred_list, average='weighted')\n",
    "svc_lin_f1_mic = f1_score(test_target, svc_lin_pred_list, average='micro')\n",
    "svc_lin_f1_mac = f1_score(test_target, svc_lin_pred_list, average='weighted')\n",
    "\n",
    "svc_lin_res = [\"SVC Linear\", svc_lin_acc, svc_lin_prec_mic, svc_lin_prec_mac, svc_lin_rec_mic, svc_lin_rec_mac, svc_lin_f1_mic, svc_lin_f1_mac]\n",
    "svc_lin_df = pd.DataFrame([svc_lin_res], columns=result_df_columns)\n",
    "\n",
    "clf_cmp_dataframe = clf_cmp_dataframe.append(svc_lin_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF\n",
    "\n",
    "rf_pred_list = rf_clf.predict(test_features)\n",
    "rf_acc = accuracy_score(test_target, rf_pred_list)\n",
    "rf_prec_mic = precision_score(test_target, rf_pred_list, average='micro')\n",
    "rf_prec_mac = precision_score(test_target, rf_pred_list, average='weighted')\n",
    "rf_rec_mic = recall_score(test_target, rf_pred_list, average='micro')\n",
    "rf_rec_mac = recall_score(test_target, rf_pred_list, average='weighted')\n",
    "rf_f1_mic = f1_score(test_target, rf_pred_list, average='micro')\n",
    "rf_f1_mac = f1_score(test_target, rf_pred_list, average='weighted')\n",
    "\n",
    "rf_res = [\"RF\", rf_acc, rf_prec_mic, rf_prec_mac, rf_rec_mic, rf_rec_mac, rf_f1_mic, rf_f1_mac]\n",
    "rf_df = pd.DataFrame([rf_res], columns=result_df_columns)\n",
    "\n",
    "clf_cmp_dataframe = clf_cmp_dataframe.append(rf_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR\n",
    "\n",
    "lr_pred_list = lr_clf.predict(test_features)\n",
    "lr_acc = accuracy_score(test_target, lr_pred_list)\n",
    "lr_prec_mic = precision_score(test_target, lr_pred_list, average='micro')\n",
    "lr_prec_mac = precision_score(test_target, lr_pred_list, average='weighted')\n",
    "lr_rec_mic = recall_score(test_target, lr_pred_list, average='micro')\n",
    "lr_rec_mac = recall_score(test_target, lr_pred_list, average='weighted')\n",
    "lr_f1_mic = f1_score(test_target, lr_pred_list, average='micro')\n",
    "lr_f1_mac = f1_score(test_target, lr_pred_list, average='weighted')\n",
    "\n",
    "lr_res = [\"LR\", lr_acc, lr_prec_mic, lr_prec_mac, lr_rec_mic, lr_rec_mac, lr_f1_mic, lr_f1_mac]\n",
    "lr_df = pd.DataFrame([lr_res], columns=result_df_columns)\n",
    "\n",
    "clf_cmp_dataframe = clf_cmp_dataframe.append(lr_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (Micro)</th>\n",
       "      <th>Precision (Weighted)</th>\n",
       "      <th>Recall (Micro)</th>\n",
       "      <th>Recall (Weighted)</th>\n",
       "      <th>F-1 (Micro)</th>\n",
       "      <th>F1 (Weighted)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC Linear</td>\n",
       "      <td>0.894290</td>\n",
       "      <td>0.894290</td>\n",
       "      <td>0.927363</td>\n",
       "      <td>0.894290</td>\n",
       "      <td>0.894290</td>\n",
       "      <td>0.894290</td>\n",
       "      <td>0.904427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.996835</td>\n",
       "      <td>0.996835</td>\n",
       "      <td>0.996359</td>\n",
       "      <td>0.996835</td>\n",
       "      <td>0.996835</td>\n",
       "      <td>0.996835</td>\n",
       "      <td>0.996556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.866415</td>\n",
       "      <td>0.866415</td>\n",
       "      <td>0.811387</td>\n",
       "      <td>0.866415</td>\n",
       "      <td>0.866415</td>\n",
       "      <td>0.866415</td>\n",
       "      <td>0.831161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Classifier  Accuracy  Precision (Micro)  Precision (Weighted)  \\\n",
       "0  SVC Linear  0.894290           0.894290              0.927363   \n",
       "1          RF  0.996835           0.996835              0.996359   \n",
       "2          LR  0.866415           0.866415              0.811387   \n",
       "\n",
       "   Recall (Micro)  Recall (Weighted)  F-1 (Micro)  F1 (Weighted)  \n",
       "0        0.894290           0.894290     0.894290       0.904427  \n",
       "1        0.996835           0.996835     0.996835       0.996556  \n",
       "2        0.866415           0.866415     0.866415       0.831161  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_cmp_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - Evaluating Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model to Choose\n",
    "\n",
    "As seen from the data above, the Random Forest model performs better than the other models in every category. This could be possible due to the fact that this dataset is a multi-class problem, which is not equally distributed amongst every class. Having an ensemble of decision trees to form a Random Forest allows for different cases to be explored, and build a better model to predict by. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Metrics\n",
    "\n",
    "#### Precision and Recall Tradeoff  (Using a Precision Recall Curve)\n",
    "When evaluating models, a common dilemma is the trade-off between precision and recall. Ideally, we would want high precision and high recall, but at times one may be boosted at the expense of the other. From a binary context, precision represents the proportion of true positives out of all the points classified as positive (true and false positives), while recall represents the proportion of true positives out of all the points that should be positive (true positives and false negatives). As such, high precision means that there are very few false positives, while high recall means most of the positives were correctly identified. However, by considering only precision or only recall, models can be created to circumvent issues to falsely project a good model. For example, by identifying every single data point as positive, there would be a perfect recall, at the expense of many false positives and low precision. Likewise, by not identifying a single positive, the inverse happens.\n",
    "\n",
    "With a precision and recall curve, we can try to identify a specific point where there is a sufficient balance between the precision and recall, where this threshold allows for suitably high precision and recall. Something similar can also be achieved using a Receiver Operating Characteristic Curve, by plotting the True Positive Rate against the False Positive Rate. In order to simplify the process, I would be considered a binary classifier instead, where the comparison would be between 'normal' and all other attack types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning attack types into binary\n",
    "\n",
    "train_target_bin = train_target.apply(lambda x: 'not normal' if x != 'normal' else x)\n",
    "test_target_bin = test_target.apply(lambda x: 0 if x != 'normal' else 1)\n",
    "\n",
    "# Creating a new RF classier\n",
    "\n",
    "rf_binary_clf = RandomForestClassifier()\n",
    "rf_binary_clf.fit(train_features, train_target_bin)\n",
    "test_score_bin = rf_binary_clf.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999801420859201\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "norm_probas = []\n",
    "for x in range(len(test_score_bin)):\n",
    "    norm_probas = norm_probas + [test_score_bin[x][0]]\n",
    "\n",
    "avg_precision_score = average_precision_score(test_target_bin, norm_probas) \n",
    "print avg_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'2-class Precision-Recall curve: AP=1.00')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Following scikit-learn tutorial\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precision, recall, k = precision_recall_curve(test_target_bin, norm_probas)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          avg_precision_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k)):\n",
    "    if precision[i] == 1:\n",
    "        print(\"k: \" + str(k[i]))\n",
    "        print(\"Precision: \" + str(precision[i]))\n",
    "        print(\"Recall: \" + str(recall[i]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "From the curve plotted above, I have chosen a k-value of 0.889 , which offers a precision score of 1.0 and a recall score of 0.989 . However, this curve has proven to be somewhat trivial, as the model has performed too well. This has led to high precision and recall at almost every point. As such, there is not much trade-off to be made, as the precision and recall would be high no matter which point I choose.\n",
    "\n",
    "In a less ideal scenario, I would choose a score skewed towards recall over precision, due to the purpose of the classifier. As the data involved pertains to potential attacks on a network system, it is important to take necessary precautions to mitigate any potential damage. In such a situation, it may be beneficial to allow for more False Positives as compared to False Negatives. This is to ensure that any potential risks can be explored, where it is better to realise that there is nothing wrong as compared to not detecting an attack, and realising when it is too late. Hence, my choice would be skewed towards recall, where the proportions of False Negatives are as low as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Using K-Fold CV\n",
    "\n",
    "In order to ensure that the results obtained were representative, I have used K-Folds cross-validation to check if the results obtained are similar using different folds of the data instead.\n",
    "Cross-validation is necessary to validate the model on other sets of data than the training set to ensure as little bias, and K-folds cross-validation is one of the more popular ways to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9970306322088398"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "rf2_clf = RandomForestClassifier()\n",
    "\n",
    "np.mean(cross_val_score(rf2_clf, combined_table_features, combined_table_target, cv=kf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value obtained is very similar to the one above, which validates the appropriateness and accuracy of the RF classifier on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Iterations to Improve Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Data/ Pre-Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling\n",
    "I have chosen a MinMaxScaler to use, instead of the similar MaxAbsScaler. MinMaxScaler scales all features in the range [0,1], which helps the performance of certain models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9912469701050364"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "train_feats_scaled = scaler.fit_transform(train_features.values)\n",
    "train_features.iloc[:,:] = train_feats_scaled\n",
    "\n",
    "test_feats_scaled = scaler.fit_transform(test_features.values)\n",
    "test_features.iloc[:,:] = test_feats_scaled\n",
    "\n",
    "rf_scaler_clf = RandomForestClassifier()\n",
    "\n",
    "rf_scaler_clf.fit(train_features, train_target)\n",
    "rf_pred_list_scaler = rf_scaler_clf.predict(test_features)\n",
    "\n",
    "recall_score(test_target, rf_pred_list_scaler, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite scaling being supposed to help us pefrom better, this random classifier has ended up doing worse than the original in Section 1. Hence, this model can be discarded as it isn't as useful as the other. This is due to the fact that such scaling should not really impact decision-based tree models, as they consider one variable at a time. Random Forest is implemented via multiple decision trees, hence the scaling does not have much of an impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Binary Classification on a Multi-Class Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the classifier's aim is to detect attacks from normal network accesses, we can modify the evaluation metrics to treat it as a binary classifier. This looser definition would allow for us to focus on attacks being identified, rather than the type of attack. This still serves our main purpose, where the broad goal is to distinguish attacks from normal accesses, to minimise the risks of attacks going undetected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.997481108312\n"
     ]
    }
   ],
   "source": [
    "# Calculating Recall of non-normal\n",
    "\n",
    "size_test_list = len(rf_pred_list)\n",
    "n_normal = 0\n",
    "retrieved_and_relevant = 0\n",
    "\n",
    "for x in range(size_test_list):\n",
    "    if test_target.values[x] == 'normal':\n",
    "        n_normal = n_normal + 1\n",
    "    elif rf_pred_list[x] != 'normal':\n",
    "        retrieved_and_relevant = retrieved_and_relevant + 1\n",
    "\n",
    "relevant = size_test_list - n_normal\n",
    "binary_recall = retrieved_and_relevant / float(relevant)\n",
    "print(binary_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value above shows that the multi-class model of the RF can be adopted to be treated as a binary classifier, as there is a high recall which minimises the number of attacks that go undetected. Albeit some of the attacks may be wrongly identified as another form of attack, the classifier would still flag them for investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Grid Search CV to Find Best RF Hyper-parameters\n",
    "While the default RF classifier performed extremely well already, I will use Grid Search CV to see if it can improve on the default model above. The default uses n_estimators at 10, and max_depth at None. These are the 2 hyper-parameters which I will be varying in order to observe the difference it has on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 200, 'max_depth': None}\n",
      "0.9962892027771127\n"
     ]
    }
   ],
   "source": [
    "parameters = {'n_estimators':[10,50,100,200], 'max_depth':[None,3,5,10,15,20,25,30,35,40]}\n",
    "rf_clf3 = RandomForestClassifier()\n",
    "cv_clf_2 = GridSearchCV(rf_clf3, parameters)\n",
    "cv_clf_2.fit(train_features, train_target)\n",
    "\n",
    "print(cv_clf_2.best_params_)\n",
    "print(cv_clf_2.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above shows similiar results in our model when we tweak the hyper-parameters passed into the Random Forest classifier. Having said that, a marginal gain is still an improvement that we can adopt to better our choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 - Limitations and Other Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Imbalanced Dataset : \n",
    "    The data set does not reflect the true nature of how network attacks take place, as the data represents close to a 50/50 split between \"normal\" and attacks. This would signify more of a binary or multi-class classifier, rather than an anomaly detector. In reality, the majority of network traffic should be \"normal\", which makes it harder for a true classifier to be able to be trained to do such detections of attack.\n",
    "    \n",
    "2) Clean Dataset :\n",
    "    The data set also contains clean data, where every field is filled-in. This does not accurately reflect real-life scenarios, where the data may have to undergo major pre-processing before sense can be made of it. The data also came in with mostly all the features labelled, which aids manual feature selection/creation.\n",
    "    \n",
    "3) Academic Dataset :\n",
    "    In addition to the points above about the dataset, it must also be noted that it is an academic dataset, that has been well-researched and explored previously. Even without any domain knowledge, the abundance of academic reports available online aided my ability to make better and well-informed decisions in terms of models to run, and what to expect.\n",
    "\n",
    "4) Choice of models :\n",
    "    In Section 1, I only considered Linear SVC, Random Forest, and Logistic Regressions, and this could have limited my results by not choosing other models to run as well. However, the costs of running more expensive models would not be worth the marginal gains over the results the Random Forest already obtained. The Linear SVC took 13 mins to fit and train the model, and this is already a lot quicker than if I did a SVC with a linear kernel instead. Hence, possibly better models such as SVC with RBF kernel and Deep Neural Networks were not considered in my exploration.\n",
    "\n",
    "5) Lack of Further Iterations :\n",
    "    Having found in Section 1 that Random Forest performs a fair bit better than Linear SVC and Logistic Regression, I chose to focus only on Random Forest in further iterations. Perhaps with further tweaking of the other models, improvements could have been found in the other models to become comparable with the Random Forest ones. Due to a lack of time and the costs of running GridSearchCV on models such as Linear SVC, I assumed that Random Forest would be the best model still, as it was already close to 100%.\n",
    "\n",
    "6) Ordinality Introduced in Enumeration :\n",
    "    When enumerating the categoricals in order to give them numerical values for the classifiers, a false ordinality was introduced in those features which may skew the result. The other option would be to use scikit-learn's one hot encoding, or numpy.eye, which would introduce the curse of dimensionality instead. \n",
    "    \n",
    "7) Feature Engineering :\n",
    "    As the results obtained via the Random Forest is already very good, there is little room for feature engineering, be it selection or creation, to improve the model. This might not be the case in other datasets, where methods such as recursive feature selection can be used to rank features. Other aspects of feature selection, such as correlation feature selection, or using the filter or wrapper method, can also be considered in other cases. Manual analysing and/or domain knowledge in other cases may also aid feature creation based on the data given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 - Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, I've found that Random Forest outperforms both the Linear SVC and Logistic Regression in creating a Multi-Class classifier to distinguish attacks in the NSL-KDD dataset. Through further iterations, there seemed to be only marginal gains as compared to the default model, as the default model had already performed really well (99.5%). It must also be noted that due to time and costs, other potentially better models such as the SVC with a RBF kernel were not tested. Given how well the default Random Forest has done though, it would suffice to just adopt it as the go-to model for this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
